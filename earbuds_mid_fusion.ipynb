{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27965532",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbf037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# midfusion_train_complete_fixed.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1ac7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Dataset ────────────────────────────────────────────────────────────────────\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RGBDDetectionDataset(Dataset):\n",
    "    def __init__(self, yaml_path: str, split: str = \"train\", transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            yaml_path: Path to dataset.yaml file\n",
    "            split: \"train\", \"val\", or \"test\"\n",
    "            transforms: Optional transforms\n",
    "        \"\"\"\n",
    "        # Load YAML\n",
    "        yaml_path = Path(yaml_path)\n",
    "        if not yaml_path.exists():\n",
    "            raise FileNotFoundError(f\"YAML file not found: {yaml_path}\")\n",
    "            \n",
    "        with open(yaml_path, \"r\") as f:\n",
    "            data_yaml = yaml.safe_load(f)\n",
    "\n",
    "        if split not in data_yaml:\n",
    "            raise ValueError(f\"Split '{split}' not found in YAML. Available: {list(data_yaml.keys())}\")\n",
    "\n",
    "        # Get the base directory (where YAML is located)\n",
    "        base_dir = yaml_path.parent\n",
    "        \n",
    "        # Try different path resolution methods\n",
    "        split_path_str = data_yaml[split]\n",
    "        \n",
    "        # Method 1: Relative to YAML file\n",
    "        if split_path_str.startswith('../'):\n",
    "            image_dir = base_dir / split_path_str\n",
    "        # Method 2: Absolute path\n",
    "        elif Path(split_path_str).is_absolute():\n",
    "            image_dir = Path(split_path_str)\n",
    "        # Method 3: Relative to base directory\n",
    "        else:\n",
    "            image_dir = base_dir / split_path_str\n",
    "            \n",
    "        # If the path points to images directory directly, use it\n",
    "        if image_dir.name == 'images' and image_dir.exists():\n",
    "            self.image_dir = image_dir\n",
    "            split_dir = image_dir.parent\n",
    "        # If it points to split directory, append images\n",
    "        elif (image_dir / 'images').exists():\n",
    "            split_dir = image_dir\n",
    "            self.image_dir = split_dir / 'images'\n",
    "        # Try going up one level and finding the split\n",
    "        else:\n",
    "            split_dir = base_dir / split\n",
    "            self.image_dir = split_dir / 'images'\n",
    "            \n",
    "        print(f\"Looking for images in: {self.image_dir}\")\n",
    "        \n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Images directory not found: {self.image_dir}\")\n",
    "\n",
    "        # Find all image files\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "        self.rgb_paths = []\n",
    "        for ext in image_extensions:\n",
    "            self.rgb_paths.extend(self.image_dir.glob(f\"*{ext}\"))\n",
    "            self.rgb_paths.extend(self.image_dir.glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        self.rgb_paths = sorted(self.rgb_paths)\n",
    "        \n",
    "        if len(self.rgb_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {self.image_dir}\")\n",
    "\n",
    "        # Set up other directories\n",
    "        self.depth_dir = split_dir / \"depth\"\n",
    "        self.label_dir = split_dir / \"labels\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Class names and count\n",
    "        self.names = data_yaml.get(\"names\", [])\n",
    "        self.nc = data_yaml.get(\"nc\", len(self.names))\n",
    "        \n",
    "        print(f\"Found {len(self.rgb_paths)} images\")\n",
    "        print(f\"Depth dir: {self.depth_dir} (exists: {self.depth_dir.exists()})\")\n",
    "        print(f\"Label dir: {self.label_dir} (exists: {self.label_dir.exists()})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path   = self.rgb_paths[idx]\n",
    "        stem       = rgb_path.stem\n",
    "        depth_path = self.depth_dir / f\"{stem}.png\"\n",
    "        label_path = self.label_dir / f\"{stem}.txt\"\n",
    "\n",
    "        # Load RGB\n",
    "        img = cv2.imread(str(rgb_path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load RGB image: {rgb_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        # Load Depth\n",
    "        if depth_path.exists():\n",
    "            d = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)\n",
    "            if d is None:\n",
    "                print(f\"Warning: Could not load depth image: {depth_path}, using zeros\")\n",
    "                d = np.zeros(img.shape[:2], dtype=np.float32)\n",
    "            else:\n",
    "                d = d.astype(np.float32)\n",
    "                if len(d.shape) == 3 and d.shape[2] == 3:\n",
    "                    d = cv2.cvtColor(d, cv2.COLOR_RGB2GRAY)\n",
    "                elif len(d.shape) == 3 and d.shape[2] == 1:\n",
    "                    d = d.squeeze(-1)\n",
    "                d /= (d.max() + 1e-6)\n",
    "        else:\n",
    "            print(f\"Warning: Depth file not found: {depth_path}, using zeros\")\n",
    "            d = np.zeros(img.shape[:2], dtype=np.float32)\n",
    "            \n",
    "        d = d[..., None]   # H×W×1\n",
    "\n",
    "        # Load Labels (YOLO txt format: cls xc yc w h)\n",
    "        H, W = img.shape[:2]\n",
    "        boxes = []\n",
    "        if label_path.exists():\n",
    "            try:\n",
    "                with open(label_path) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            parts = line.split()\n",
    "                            if len(parts) >= 5:\n",
    "                                cls, xc, yc, w, h = map(float, parts[:5])\n",
    "                                x0 = max(0, (xc - w/2) * W)\n",
    "                                y0 = max(0, (yc - h/2) * H)\n",
    "                                x1 = min(W, (xc + w/2) * W)\n",
    "                                y1 = min(H, (yc + h/2) * H)\n",
    "                                if x1 > x0 and y1 > y0:  # Valid box\n",
    "                                    boxes.append([x0, y0, x1, y1, int(cls)])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading label file {label_path}: {e}\")\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "\n",
    "        # Convert to tensors\n",
    "        rgb_t   = torch.from_numpy(img.transpose(2, 0, 1))   # 3×H×W\n",
    "        depth_t = torch.from_numpy(d.transpose(2, 0, 1))     # 1×H×W\n",
    "\n",
    "        target = {\n",
    "            \"boxes\":  boxes[:, :4],\n",
    "            \"labels\": boxes[:, 4].long() + 1   # shift classes for FasterRCNN (0=background)\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            rgb_t, depth_t, target = self.transforms(rgb_t, depth_t, target)\n",
    "\n",
    "        return rgb_t, depth_t, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee89d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Collate Function ───────────────────────────────────────────────────────\n",
    "\n",
    "def collate_fn(batch):\n",
    "    rgbs, depths, targets = zip(*batch)\n",
    "    return list(rgbs), list(depths), list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf49b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Mid‐Level Fusion Backbone ─────────────────────────────────────────────────\n",
    "\n",
    "class MidLevelFusionBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # RGB ResNet50\n",
    "        res_r = resnet50(pretrained=True)\n",
    "        # Depth ResNet50 (single-channel)\n",
    "        res_d = resnet50(pretrained=False)\n",
    "        res_d.conv1 = nn.Conv2d(1, res_d.conv1.out_channels,\n",
    "                                kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        return_layers = {\"layer1\":\"c2\", \"layer2\":\"c3\", \"layer3\":\"c4\", \"layer4\":\"c5\"}\n",
    "        self.rgb_extractor   = IntermediateLayerGetter(res_r, return_layers)\n",
    "        self.depth_extractor = IntermediateLayerGetter(res_d, return_layers)\n",
    "\n",
    "        # FPN for concatenated features (C2–C5)\n",
    "        in_chs = [256*2, 512*2, 1024*2, 2048*2]\n",
    "        self.fpn = FeaturePyramidNetwork(in_channels_list=in_chs, out_channels=256)\n",
    "        self.out_channels = 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x may be a list of Tensors [4×H×W] or a single Tensor [B,4,H,W]\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = torch.stack(x, dim=0)\n",
    "        # now x: [B,4,H,W]\n",
    "        rgb   = x[:, :3, :, :]\n",
    "        depth = x[:, 3:4, :, :]\n",
    "\n",
    "        feats_r = self.rgb_extractor(rgb)\n",
    "        feats_d = self.depth_extractor(depth)\n",
    "        fused   = {name: torch.cat([feats_r[name], feats_d[name]], dim=1)\n",
    "                   for name in feats_r}\n",
    "        return self.fpn(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ─── Model Factory ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_midfusion_fasterrcnn(num_classes: int):\n",
    "    backbone = MidLevelFusionBackbone()\n",
    "\n",
    "    # 4 FPN levels → one size and aspect_ratios tuple per level\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32,), (64,), (128,), (256,)),\n",
    "        aspect_ratios=((0.5,1.0,2.0),) * 4\n",
    "    )\n",
    "\n",
    "    # ROI pooling on the 4 FPN maps: c2–c5\n",
    "    roi_pool = MultiScaleRoIAlign(\n",
    "        featmap_names=['c2','c3','c4','c5'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pool\n",
    "    )\n",
    "    # Normalize 4th channel (depth) with zero-mean\n",
    "    model.transform.image_mean = [0.485, 0.456, 0.406, 0.0]\n",
    "    model.transform.image_std  = [0.229, 0.224, 0.225, 1.0]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c15e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Training & Early Stopping ─────────────────────────────────────────────────\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [train]\", unit=\"batch\")\n",
    "    total = 0.0\n",
    "    for rgbs, depths, targets in pbar:\n",
    "        inputs = [torch.cat([r, d], dim=0).to(device)\n",
    "                  for r, d in zip(rgbs, depths)]\n",
    "        tgts = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(inputs, tgts)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
    "    pbar.close()\n",
    "    return total / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303f8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss(model, loader, device):\n",
    "    # keep train() so loss_dict is returned\n",
    "    model.train()\n",
    "    pbar = tqdm(loader, desc=\"[val]  \", unit=\"batch\")\n",
    "    total = 0.0\n",
    "    for rgbs, depths, targets in pbar:\n",
    "        inputs = [torch.cat([r, d], dim=0).to(device)\n",
    "                  for r, d in zip(rgbs, depths)]\n",
    "        tgts   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss = sum(model(inputs, tgts).values()).item()\n",
    "        total += loss\n",
    "        pbar.set_postfix(val_loss=f\"{loss:.4f}\")\n",
    "    pbar.close()\n",
    "    return total / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b2a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in: dataset\\train\\images\n",
      "Found 126 images\n",
      "Depth dir: dataset\\train\\depth (exists: True)\n",
      "Label dir: dataset\\train\\labels (exists: True)\n",
      "Looking for images in: dataset\\val\\images\n",
      "Found 32 images\n",
      "Depth dir: dataset\\val\\depth (exists: True)\n",
      "Label dir: dataset\\val\\labels (exists: True)\n",
      "Looking for images in: dataset\\test\\images\n",
      "Found 18 images\n",
      "Depth dir: dataset\\test\\depth (exists: True)\n",
      "Label dir: dataset\\test\\labels (exists: True)\n",
      "4\n",
      "Dataset sizes - Train: 126, Val: 32, Test: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1 [train]: 100%|██████████| 16/16 [00:59<00:00,  3.75s/batch, train_loss=0.4407]\n",
      "[val]  : 100%|██████████| 4/4 [00:00<00:00,  4.51batch/s, val_loss=0.4446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train_loss=1.0641, val_loss=0.5325\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [train]: 100%|██████████| 16/16 [01:05<00:00,  4.07s/batch, train_loss=0.3756]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.90batch/s, val_loss=0.3867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: train_loss=0.3601, val_loss=0.4335\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [train]: 100%|██████████| 16/16 [01:07<00:00,  4.20s/batch, train_loss=0.2165]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.94batch/s, val_loss=0.2724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: train_loss=0.2470, val_loss=0.3347\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [train]: 100%|██████████| 16/16 [00:59<00:00,  3.74s/batch, train_loss=0.1692]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.85batch/s, val_loss=0.2559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: train_loss=0.2024, val_loss=0.3067\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [train]: 100%|██████████| 16/16 [01:01<00:00,  3.84s/batch, train_loss=0.1265]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.79batch/s, val_loss=0.2416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: train_loss=0.1637, val_loss=0.2934\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [train]: 100%|██████████| 16/16 [01:03<00:00,  3.99s/batch, train_loss=0.1265]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.01batch/s, val_loss=0.2263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: train_loss=0.1518, val_loss=0.3297\n",
      "  ↳ No improvement for 1/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [train]: 100%|██████████| 16/16 [01:00<00:00,  3.80s/batch, train_loss=0.1078]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.44batch/s, val_loss=0.1796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: train_loss=0.1328, val_loss=0.2586\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [train]: 100%|██████████| 16/16 [01:02<00:00,  3.93s/batch, train_loss=0.1268]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.01batch/s, val_loss=0.2136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: train_loss=0.1173, val_loss=0.2886\n",
      "  ↳ No improvement for 1/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [train]: 100%|██████████| 16/16 [01:04<00:00,  4.03s/batch, train_loss=0.1181]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.98batch/s, val_loss=0.2303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: train_loss=0.1080, val_loss=0.2933\n",
      "  ↳ No improvement for 2/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [train]: 100%|██████████| 16/16 [00:59<00:00,  3.72s/batch, train_loss=0.0968]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.74batch/s, val_loss=0.1837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train_loss=0.0964, val_loss=0.2572\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [train]: 100%|██████████| 16/16 [01:01<00:00,  3.82s/batch, train_loss=0.0715]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.95batch/s, val_loss=0.1706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train_loss=0.0798, val_loss=0.2551\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [train]: 100%|██████████| 16/16 [01:03<00:00,  3.98s/batch, train_loss=0.0546]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.92batch/s, val_loss=0.2269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train_loss=0.0758, val_loss=0.3049\n",
      "  ↳ No improvement for 1/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [train]: 100%|██████████| 16/16 [01:05<00:00,  4.08s/batch, train_loss=0.0668]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.99batch/s, val_loss=0.1712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train_loss=0.0827, val_loss=0.2473\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [train]: 100%|██████████| 16/16 [01:02<00:00,  3.90s/batch, train_loss=0.0828]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.96batch/s, val_loss=0.1508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train_loss=0.0804, val_loss=0.2493\n",
      "  ↳ No improvement for 1/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [train]: 100%|██████████| 16/16 [01:04<00:00,  4.05s/batch, train_loss=0.0621]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.94batch/s, val_loss=0.1805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train_loss=0.0677, val_loss=0.2431\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [train]: 100%|██████████| 16/16 [01:06<00:00,  4.14s/batch, train_loss=0.0528]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.92batch/s, val_loss=0.1651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train_loss=0.0608, val_loss=0.2408\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [train]: 100%|██████████| 16/16 [01:06<00:00,  4.16s/batch, train_loss=0.0748]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.91batch/s, val_loss=0.1758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train_loss=0.0604, val_loss=0.2619\n",
      "  ↳ No improvement for 1/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [train]: 100%|██████████| 16/16 [01:01<00:00,  3.84s/batch, train_loss=0.0605]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  2.94batch/s, val_loss=0.1793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train_loss=0.0623, val_loss=0.2444\n",
      "  ↳ No improvement for 2/5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [train]: 100%|██████████| 16/16 [01:03<00:00,  3.97s/batch, train_loss=0.0404]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.68batch/s, val_loss=0.1558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train_loss=0.0566, val_loss=0.2262\n",
      "  ↳ Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [train]: 100%|██████████| 16/16 [01:01<00:00,  3.81s/batch, train_loss=0.0590]\n",
      "[val]  : 100%|██████████| 4/4 [00:01<00:00,  3.71batch/s, val_loss=0.1643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_loss=0.0638, val_loss=0.2424\n",
      "  ↳ No improvement for 1/5 epochs.\n",
      "Training complete. Best val_loss: 0.2262\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[val]  : 100%|██████████| 3/3 [00:00<00:00,  4.68batch/s, val_loss=0.0966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ── Hyperparameters ─────────────────────────────────────────────────────────\n",
    "    YAML_PATH   = \"dataset/data.yaml\"   # path to dataset.yaml\n",
    "    BATCH_SIZE  = 8\n",
    "    LR          = 1e-4\n",
    "    WD          = 1e-4\n",
    "    MAX_EPOCHS  = 20\n",
    "    PATIENCE    = 5\n",
    "    SEED        = 42\n",
    "    DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # Create datasets for each split\n",
    "    train_ds = RGBDDetectionDataset(YAML_PATH, split=\"train\")\n",
    "    val_ds   = RGBDDetectionDataset(YAML_PATH, split=\"val\")\n",
    "    test_ds  = RGBDDetectionDataset(YAML_PATH, split=\"test\")\n",
    "\n",
    "    # Number of classes (add 1 for background)\n",
    "    NUM_CLASSES = train_ds.nc + 1  \n",
    "    print(NUM_CLASSES)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    model     = build_midfusion_fasterrcnn(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    best_val_loss     = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, DEVICE, epoch)\n",
    "        vl_loss = eval_loss(model, val_loader, DEVICE)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: train_loss={tr_loss:.4f}, val_loss={vl_loss:.4f}\")\n",
    "\n",
    "        if vl_loss < best_val_loss:\n",
    "            best_val_loss     = vl_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"midfusion_best1.pth\")\n",
    "            print(\"  ↳ Validation loss improved; model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  ↳ No improvement for {epochs_no_improve}/{PATIENCE} epochs.\")\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Training complete. Best val_loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Optional: Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(torch.load(\"midfusion_best1.pth\"))\n",
    "    test_loss = eval_loss(model, test_loader, DEVICE)\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ea4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_for_confusion_matrix(model, dataloader, device, \n",
    "                                      iou_threshold=0.5, confidence_threshold=0.5):\n",
    "    \"\"\"Evaluate model and return predictions and targets for confusion matrix\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    for batch_idx, (rgbs, depths, targets) in enumerate(tqdm(dataloader)):\n",
    "        # Prepare inputs\n",
    "        inputs = [torch.cat([r, d], dim=0).to(device) for r, d in zip(rgbs, depths)]\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Process each image in batch\n",
    "        for output, target in zip(outputs, targets):\n",
    "            pred_boxes = output['boxes']\n",
    "            pred_labels = output['labels']  \n",
    "            pred_scores = output['scores']\n",
    "            \n",
    "            target_boxes = target['boxes'].to(device)\n",
    "            target_labels = target['labels'].to(device)\n",
    "            \n",
    "            # Filter predictions by confidence\n",
    "            valid_preds = pred_scores >= confidence_threshold\n",
    "            pred_boxes = pred_boxes[valid_preds]\n",
    "            pred_labels = pred_labels[valid_preds]\n",
    "            pred_scores = pred_scores[valid_preds]\n",
    "            \n",
    "            # Match predictions to targets\n",
    "            used_targets = set()\n",
    "            \n",
    "            # For each prediction, find best matching target\n",
    "            for pred_box, pred_label in zip(pred_boxes, pred_labels):\n",
    "                best_iou = 0\n",
    "                best_target_idx = -1\n",
    "                \n",
    "                for j, (target_box, target_label) in enumerate(zip(target_boxes, target_labels)):\n",
    "                    if j in used_targets:\n",
    "                        continue\n",
    "                    \n",
    "                    iou = calculate_iou(pred_box.cpu().numpy(), target_box.cpu().numpy())\n",
    "                    if iou > best_iou and iou >= iou_threshold:\n",
    "                        best_iou = iou\n",
    "                        best_target_idx = j\n",
    "                \n",
    "                if best_target_idx != -1:\n",
    "                    # True positive - correct detection\n",
    "                    all_preds.append(pred_label.item())\n",
    "                    all_targets.append(target_labels[best_target_idx].item())\n",
    "                    used_targets.add(best_target_idx)\n",
    "                else:\n",
    "                    # False positive - predicted something that doesn't exist\n",
    "                    all_preds.append(pred_label.item())\n",
    "                    all_targets.append(0)  # Background class for FP\n",
    "            \n",
    "            # Add unmatched targets as false negatives\n",
    "            for j, target_label in enumerate(target_labels):\n",
    "                if j not in used_targets:\n",
    "                    all_preds.append(0)  # Background class for FN\n",
    "                    all_targets.append(target_label.item())\n",
    "    \n",
    "    return all_preds, all_targets\n",
    "\n",
    "def plot_confusion_matrix(all_preds, all_targets, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    # Create class names including background\n",
    "    full_class_names = ['Background'] + class_names\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds, \n",
    "                         labels=range(len(full_class_names)))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=full_class_names,\n",
    "               yticklabels=full_class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 60)\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        labels=range(len(full_class_names)),\n",
    "        target_names=full_class_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be39750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['bud', 'mouse', 'stepler']\n",
      "Number of classes (including background): 4\n",
      "Creating model architecture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "v:\\model\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained weights...\n",
      "✓ Model loaded successfully!\n",
      "Loading test dataset...\n",
      "Looking for images in: dataset\\test\\images\n",
      "Found 18 images\n",
      "Depth dir: dataset\\test\\depth (exists: True)\n",
      "Label dir: dataset\\test\\labels (exists: True)\n",
      "Test dataset size: 18\n",
      "Generating confusion matrix...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.26it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Background       0.00      0.00      0.00         0\n",
      "         bud       1.00      1.00      1.00         6\n",
      "       mouse       1.00      1.00      1.00         6\n",
      "     stepler       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00        18\n",
      "   macro avg       0.75      0.75      0.75        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "Confusion matrix generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Complete code for loading model and generating confusion matrix\n",
    "\n",
    "# Step 1: Load configuration\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "YAML_PATH = \"dataset/data.yaml\"\n",
    "\n",
    "# Load class information\n",
    "try:\n",
    "    with open(YAML_PATH, 'r') as f:\n",
    "        data_yaml = yaml.safe_load(f)\n",
    "    class_names = data_yaml['names']\n",
    "    NUM_CLASSES = data_yaml['nc'] + 1  # +1 for background\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Number of classes (including background): {NUM_CLASSES}\")\n",
    "except:\n",
    "    # Fallback if YAML not available\n",
    "    class_names = ['class1', 'class2', 'class3']  # Replace with your actual class names\n",
    "    NUM_CLASSES = 4  # Replace with your actual number (including background)\n",
    "    print(f\"Using fallback classes: {class_names}\")\n",
    "\n",
    "# Step 2: Recreate model architecture\n",
    "print(\"Creating model architecture...\")\n",
    "model = build_midfusion_fasterrcnn(NUM_CLASSES)\n",
    "\n",
    "# Step 3: Load trained weights\n",
    "print(\"Loading trained weights...\")\n",
    "state_dict = torch.load(\"midfusion_best1.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "\n",
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_dataset = RGBDDetectionDataset(YAML_PATH, split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, \n",
    "                        collate_fn=collate_fn, num_workers=0)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "print(\"Generating confusion matrix...\")\n",
    "all_preds, all_targets = evaluate_model_for_confusion_matrix(\n",
    "    model, test_loader, DEVICE, \n",
    "    iou_threshold=0.5, \n",
    "    confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "if len(all_preds) > 0:\n",
    "    cm = plot_confusion_matrix(all_preds, all_targets, class_names)\n",
    "    print(\"Confusion matrix generated successfully!\")\n",
    "else:\n",
    "    print(\"No predictions found. Check your dataset and thresholds.\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for single image test:\n",
    "# test_single_image(model, \"path/to/rgb.png\", \"path/to/depth.png\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "708ea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test on a single image (if you want to verify the model works)\n",
    "def test_single_image(model, rgb_path, depth_path, device, save_path=None):\n",
    "    \"\"\"Test model on a single image and draw bounding boxes\"\"\"\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # Load images\n",
    "    rgb = cv2.imread(rgb_path)\n",
    "    if rgb is None:\n",
    "        print(f\"Could not load RGB image: {rgb_path}\")\n",
    "        return\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    depth = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if depth is None:\n",
    "        print(f\"Could not load depth image: {depth_path}\")\n",
    "        return\n",
    "    \n",
    "    if depth.ndim == 2:\n",
    "        depth = depth[..., None]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    rgb_t = torch.from_numpy(rgb.copy()).permute(2, 0, 1).float()\n",
    "    depth_t = torch.from_numpy(depth.copy()).permute(2, 0, 1).float()\n",
    "    \n",
    "    rgb_t = rgb_t / 255.0\n",
    "    depth_t = depth_t / 255.0\n",
    "\n",
    "\n",
    "    # Combine and add batch dimension\n",
    "    input_tensor = torch.cat([rgb_t, depth_t], dim=0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)[0]\n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    # Draw bounding boxes using OpenCV for saving\n",
    "    rgb_copy = rgb.copy()\n",
    "    for box, label, score in zip(predictions[\"boxes\"], predictions[\"labels\"], predictions[\"scores\"]):\n",
    "\n",
    "        if score < 0.6:  # Skip low confidence predictions\n",
    "            continue\n",
    "        \n",
    "        x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "        class_idx = label.item() - 1  # Convert back from FasterRCNN format\n",
    "        if 0 <= class_idx < len(class_names):\n",
    "            class_name = class_names[class_idx]\n",
    "        else:\n",
    "            class_name = f\"Class{class_idx}\"\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(rgb_copy, (x1, y1), (x2, y2), (255, 0, 0), 2)  # blue box\n",
    "        cv2.putText(rgb_copy, f\"{class_name} {score:.2f}\", \n",
    "                    (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "    \n",
    "    # Show result with matplotlib\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(rgb_copy)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Model Predictions with Bounding Boxes\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Optionally save the image\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(rgb_copy, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved output with boxes at: {save_path}\")\n",
    "    print(class_name)\n",
    "    print(f\"Found {len(predictions['boxes'])} detections\")\n",
    "    print(f\"Confidence scores: {predictions['scores'].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0877458e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 467 but got size 480 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mGuest_\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPictures\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mScreenshots\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mwork\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDepth\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdepth_4.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 32\u001b[0m, in \u001b[0;36mtest_single_image\u001b[1;34m(model, rgb_path, depth_path, device, save_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m depth_t \u001b[38;5;241m=\u001b[39m depth_t \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Combine and add batch dimension\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrgb_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 467 but got size 480 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "test_single_image(model, r\"C:\\Users\\Guest_\\Pictures\\Screenshots\\test.png\", r\"V:\\model\\temp\\work\\Depth\\depth_4.png\", DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
